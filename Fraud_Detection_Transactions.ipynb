{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMA77coM4voN69U9PLbgPQc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MatiasSiles/MastercardSalesOptimization/blob/main/Fraud_Detection_Transactions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modelo de detección temprana de anomalías y fraudes financieros multivariado con series temporales, usando Deep Learning (LSTM/Transformer) y métodos bayesianos para estimar riesgo dinámico en transacciones Mastercard a nivel global."
      ],
      "metadata": {
        "id": "8PIT7SABBpho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detección en tiempo real de fraudes invisibles de bajo monto, usando anomalías multivariada"
      ],
      "metadata": {
        "id": "qc-uUZVIB1tg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sqlite3\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "ntJrK3QbKShi"
      },
      "execution_count": 238,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "connection_db = sqlite3.connect(\"mastercard.db\")\n",
        "\n",
        "df_customers = pd.read_sql(\"SELECT * FROM customers\", connection_db)\n",
        "df_cards = pd.read_sql(\"SELECT * FROM cards\", connection_db)\n",
        "df_merchants = pd.read_sql(\"SELECT * FROM merchants\", connection_db)\n",
        "df_transactions = pd.read_sql(\"SELECT * FROM transactions\", connection_db)\n",
        "df_fraud_labels = pd.read_sql(\"SELECT * FROM fraud_labels\", connection_db)\n",
        "\n",
        "connection_db.close()"
      ],
      "metadata": {
        "id": "pRVrre9mKVg0"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class fraud_analyzer():\n",
        "  def __init__(self, * ,customers=None, transactions=None, cards=None, fraud_labels=None, merchants=None):\n",
        "    self.customers = customers\n",
        "    self.transactions = transactions\n",
        "    self.cards = cards\n",
        "    self.fraud_labels = fraud_labels\n",
        "    self.merchants = merchants\n",
        "\n",
        "  def Customers(self):\n",
        "    fig, axes = plt.subplots(3,2, figsize=(16,8))\n",
        "    fig.suptitle(\"Customers Analysis\")\n",
        "\n",
        "    # plot1\n",
        "    axes[0,0].scatter(self.customers[\"age\"], self.customers[\"income\"], alpha=0.4)\n",
        "    axes[0,0].set_xlabel(\"Age\")\n",
        "    axes[0,0].set_ylabel(\"Income\")\n",
        "\n",
        "    # plot2\n",
        "    idcustomer_idtransaction_amount = pd.merge(self.cards, self.transactions, on=\"card_id\")[[\"customer_id\", \"transaction_id\",\"amount\"]]\n",
        "    income_vs_amount = pd.merge(idcustomer_idtransaction_amount, self.customers, on=\"customer_id\")[[\"income\", \"amount\"]]\n",
        "\n",
        "    axes[0,1].scatter(income_vs_amount[\"income\"], income_vs_amount[\"amount\"], alpha=0.2)\n",
        "    axes[0,1].set_xlabel(\"Income\")\n",
        "    axes[0,1].set_ylabel(\"Transaction Amount\")\n",
        "\n",
        "    # plot3\n",
        "    axes[1,0].hist(idcustomer_idtransaction_amount[\"customer_id\"], bins=50)\n",
        "    axes[1,0].set_xlabel(\"Customer ID\")\n",
        "    axes[1,0].set_ylabel(\"Transaction Frequency\")\n",
        "\n",
        "    # plot4\n",
        "    data = pd.merge(self.cards, self.transactions, on=\"card_id\")[[\"customer_id\",\"amount\"]]\n",
        "    data = data.groupby(\"customer_id\").mean()\n",
        "\n",
        "    axes[1,1].scatter(data.index, data[\"amount\"], alpha=0.3)\n",
        "    axes[1,1].set_xlabel(\"Customer ID\")\n",
        "    axes[1,1].set_ylabel(\"Average Transaction Amount\")\n",
        "\n",
        "    # plot5\n",
        "    self.transactions[\"timestamp\"] = pd.to_datetime(self.transactions[\"timestamp\"])\n",
        "    frequency_transaction_hour = self.transactions[\"timestamp\"].dt.hour.sort_values()\n",
        "\n",
        "    axes[2,0].hist(frequency_transaction_hour)\n",
        "    axes[2,0].set_xlabel(\"Hour\")\n",
        "    axes[2,0].set_ylabel(\"Transaction Frequency\")\n",
        "\n",
        "    # plot6\n",
        "    frequency_transaction_date = self.transactions[\"timestamp\"].sort_values()\n",
        "    frequency_transaction_date = self.transactions[\"timestamp\"].dt.date\n",
        "    frequency_transaction_date = pd.to_datetime(self.transactions[\"timestamp\"])\n",
        "\n",
        "    axes[2,1].hist(frequency_transaction_date, bins=80)\n",
        "    axes[2,1].set_xlabel(\"Date\")\n",
        "    axes[2,1].set_ylabel(\"Transaction Frequency\")\n",
        "\n",
        "    fig.tight_layout()\n",
        "\n",
        "  def transaction_customer_map(self):\n",
        "    world_map = gpd.read_file(\"/content/ne_110m_admin_0_countries.shp\")\n",
        "\n",
        "    countries = self.customers[\"country\"].value_counts().reset_index().rename(columns={\"country\":\"ISO_A2\"})\n",
        "    world_map = pd.merge(world_map, countries, on=\"ISO_A2\")\n",
        "\n",
        "    world_map.plot(column=\"count\", cmap=\"OrRd\", legend=True, color=\"lightblue\", figsize=(20,8))\n",
        "\n",
        "  def fraud_distribution(self):\n",
        "    data = self.fraud_labels[\"is_fraud\"].value_counts()\n",
        "    data.plot(kind=\"bar\")\n",
        "    frauds_total = data.iloc[data.index == 1].item()\n",
        "    fraud_porcent = (frauds_total * 100) / len(self.fraud_labels)\n",
        "\n",
        "    print(data)\n",
        "    print(f\"\\nFraud Porcent: {fraud_porcent}\")\n",
        "\n",
        "  class check_customers_cards():\n",
        "    def __init__(self, cards):\n",
        "      fraud_analyzer.cards\n",
        "\n",
        "    def status():\n",
        "      data = fraud_analyzer.cards[\"status\"].value_counts().plot(kind=\"bar\")\n",
        "      plt.xlabel(\"\")\n",
        "      plt.ylabel(\"Number of Customers\")\n",
        "\n",
        "    def card_types():\n",
        "      data = fraud_analyzer.cards[\"card_type\"].value_counts().plot(kind=\"bar\")\n",
        "      plt.ylabel(\"Number of Customers\")\n",
        "\n",
        "    def seeker():\n",
        "\n",
        "      while True:\n",
        "\n",
        "        prompt = int(input(\"Enter the client id or 0 to exit: \"))\n",
        "        print()\n",
        "\n",
        "        if prompt == 0:\n",
        "          break\n",
        "\n",
        "        elif prompt in fraud_analyzer.cards[\"customer_id\"].unique():\n",
        "          print(fraud_analyzer.cards[fraud_analyzer.cards[\"customer_id\"] == prompt])\n",
        "          print(\"\\n\\n\")\n",
        "\n",
        "        else:\n",
        "          print(\"Customer not found\")\n",
        "          print(\"\\n\\n\")\n",
        "\n",
        "  class Merchants():\n",
        "    def __init__(self, merchants):\n",
        "      fraud_analyzer.merchants\n",
        "\n",
        "    def sold_categories_merchants():\n",
        "      fraud_analyzer.merchants[\"category\"].value_counts().plot(kind=\"bar\")\n",
        "\n",
        "    def most_dangerous_merchants():\n",
        "      fraud_analyzer.merchants[\"risk_score\"].plot(kind=\"hist\") # risk_score indicate how many historical frauds has every merchant\n",
        "      plt.xlabel(\"Risk Score\")\n",
        "      plt.title(\"Distribution Risks Scores by Merchants\")\n",
        "\n",
        "      print(\"\\nTop 10 most dangerous merchants:\")\n",
        "      print(fraud_analyzer.merchants.sort_values(by=\"risk_score\", ascending=False).head(10))\n",
        "\n",
        "fraud_analyzer = fraud_analyzer(customers=df_customers, transactions=df_transactions, cards=df_cards, fraud_labels=df_fraud_labels, merchants=df_merchants)"
      ],
      "metadata": {
        "id": "Q0wWSQdiOLk9",
        "cellView": "form"
      },
      "execution_count": 240,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class backend_models():\n",
        "  def __init__(self, data, model, metrics):\n",
        "    self.data = pd.read_csv(data)\n",
        "    self.model = model\n",
        "    self.metrics = metrics\n",
        "\n",
        "  def data_upload(self):\n",
        "    # self.data = pd.read_csv(self.data)\n",
        "    self.X = self.data.drop([\"customer_id\", \"card_id\",  \"merchant_id\",  \"transaction_id\", \"target\"], axis=1)\n",
        "    self.y = self.data[\"target\"]\n",
        "\n",
        "  def data_split(self):\n",
        "    self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=0.2, random_state=42)\n",
        "\n",
        "  def preprocessing(self):\n",
        "    scaler = StandardScaler()\n",
        "    self.X_train_scaled = scaler.fit_transform(self.X_train)\n",
        "    self.X_test_scaled = scaler.transform(self.X_test)\n",
        "\n",
        "  def model_train(self):\n",
        "    model_trained = self.model.fit(self.X_train_scaled, self.y_train)\n",
        "    self.model = model_trained\n",
        "\n",
        "  def model_predict(self):\n",
        "    self.prediction = self.model.predict(self.X_test_scaled)\n",
        "\n",
        "  def model_evaluate(self):\n",
        "    for metric in self.metrics:\n",
        "      print(f\"{metric}: {eval(metric)(self.y_test, self.prediction):.2f}\")\n",
        "\n",
        "  def ask_tuning(self):\n",
        "    print(\"\\n1. Yes\\n2. No\\n\")\n",
        "    choice = int(input(\"It's necessary tuning?: \"))\n",
        "\n",
        "    if choice == 1:\n",
        "      params = eval(input(\"Enter de params like a dict: \")) # eval convert string to object to execute\n",
        "      grid = GridSearchCV(self.model, params, cv=5)\n",
        "      grid.fit(self.X_train_scaled, self.y_train)\n",
        "      best_model = grid.best_estimator_\n",
        "      print(f\"best params: {grid.best_params_}\\n\")\n",
        "      print(f\"best score: {grid.best_score_}\\n\")\n",
        "\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "  def predictions(self):\n",
        "    self.X = StandardScaler().fit_transform(self.X)\n",
        "    prediction = self.model.predict(self.X)\n",
        "    self.data[\"preds_reg_logistic\"] = prediction\n",
        "    data_preditcs_for_vis = self.data[\"preds_reg_logistic\"].value_counts()\n",
        "    # data_preditcs_for_vis.plot(kind=\"bar\")\n",
        "\n",
        "    # ignore\n",
        "    self.summary_customers_frauds = self.data[self.data[\"preds_reg_logistic\"] == 1].groupby(\n",
        "        [\"customer_id\", \"card_id\", \"income\",\n",
        "         \"age\", \"issue_year\",\"target\"])[[\"merchant_id\", \"transaction_id\",\n",
        "                                         \"amount\", \"timestamp_hours\",   \"risk_score\", \"Active\", \"Blocked\", \"Expired\", \"preds_reg_logistic\"]].count()\n",
        "\n",
        "  def run_model(self):\n",
        "    self.data_upload()\n",
        "    self.data_split()\n",
        "    self.preprocessing()\n",
        "    self.model_train()\n",
        "    self.model_predict()\n",
        "    self.model_evaluate()\n",
        "    self.ask_tuning()\n",
        "\n",
        "    return self.predictions()\n",
        "\n",
        "  def run_models_comparission(self):\n",
        "    self.data_upload()\n",
        "    self.data_split()\n",
        "    self.preprocessing()\n",
        "    self.model_train()\n",
        "    self.model_predict()\n",
        "\n",
        "    return self.model_evaluate(), print() # for print metrics more ordered"
      ],
      "metadata": {
        "id": "t0NpfJdH0bP4"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class models(backend_models):\n",
        "  def __init__(self, *, data, model, metrics):\n",
        "    super().__init__(data, model, metrics)\n",
        "\n",
        "  def logistic_regression():\n",
        "    lr = backend_models(\n",
        "        data=\"/content/clean_data_customers_for_predict.csv\",\n",
        "        model=LogisticRegression(C= 10, class_weight = \"balanced\", penalty = \"l1\", solver = \"liblinear\"),\n",
        "        metrics=[\"precision_score\", \"recall_score\", \"f1_score\", \"roc_auc_score\"])\n",
        "\n",
        "    return lr.run_model()\n",
        "\n",
        "  def comparision_trees_models():\n",
        "    rf = backend_models(\n",
        "        data=\"/content/clean_data_customers_for_predict.csv\",\n",
        "        model=RandomForestClassifier(class_weight = \"balanced\"),\n",
        "        metrics=[\"precision_score\", \"recall_score\", \"f1_score\", \"roc_auc_score\"])\n",
        "\n",
        "    xgb = backend_models(\n",
        "        data=\"/content/clean_data_customers_for_predict.csv\",\n",
        "        model=XGBClassifier(),\n",
        "        metrics=[\"precision_score\", \"recall_score\", \"f1_score\", \"roc_auc_score\"])\n",
        "\n",
        "    lgbm = backend_models(\n",
        "        data=\"/content/clean_data_customers_for_predict.csv\",\n",
        "        model=LGBMClassifier(verbose=-1, class_weight = 'balanced', solver = \"liblinear\"),\n",
        "        metrics=[\"precision_score\", \"recall_score\", \"f1_score\", \"roc_auc_score\"])\n",
        "\n",
        "    return rf.run_model()\n",
        "\n",
        "  def neural_networks():\n",
        "    pass\n",
        "\n",
        "# balanced because i have 10k labels and there is 219 frauds, that's very desbalanced and probably the model don't will predict good\n",
        "# i also use \"liblinear\" for linear problems\n",
        "# C = regularized the punishment level the target labels when the model fails in some things\n",
        "# penalty l1 or l2 is the punishment type, this take into account the punishment level from C"
      ],
      "metadata": {
        "id": "b_bAeqlMp5Hu"
      },
      "execution_count": 242,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models.comparision_trees_models()"
      ],
      "metadata": {
        "id": "uqPdUJi0yfi3",
        "outputId": "225626e0-f0a3-4dae-cab0-740c8976b00c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 243,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "precision_score: 0.00\n",
            "recall_score: 0.00\n",
            "f1_score: 0.00\n",
            "roc_auc_score: 0.50\n",
            "\n",
            "1. Yes\n",
            "2. No\n",
            "\n",
            "It's necessary tuning?: 2\n"
          ]
        }
      ]
    }
  ]
}